#!/usr/bin/env bash
#
# Async fetch of internet info for later MOTD scripts.
# Writes cached values to ~/.cache/motd/internet.env
#

set -euo pipefail

cache_dir="${HOME}/.cache/motd"
cache_file="${cache_dir}/internet.env"
lock_dir="${cache_dir}/internet.lock"

command -v curl >/dev/null 2>&1 || exit 0

mkdir -p "$cache_dir"

if ! mkdir "$lock_dir" 2>/dev/null; then
    exit 0
fi

(
    set -euo pipefail

    stats_file="${cache_dir}/internet-stats.jsonl"

    # URL-specific configuration
    declare -A url_headers=(
        ["https://speed.cloudflare.com/meta"]="Referer: https://speed.cloudflare.com/"
    )

    declare -A url_extract=(
        ["https://speed.cloudflare.com/meta_pub4"]="jq -r .clientIp"
        ["https://speed.cloudflare.com/meta_pub6"]="jq -r .clientIp"
        ["https://speed.cloudflare.com/meta_isp"]="jq -r .asOrganization"
    )

    # Extract data from URL response (handles both plain text and JSON)
    extract_data() {
        local url=$1
        local category=$2
        local data_file=$3
        
        local key="${url}_${category}"
        local extractor="${url_extract[$key]:-cat}"
        
        $extractor <"$data_file" 2>/dev/null || cat "$data_file"
    }

    # Log stats entry as JSON
    log_stat() {
        local cat=$1 url=$2 ms=$3 won=$4 val=$5
        jq -cn \
            --argjson ts "$(date +%s)" \
            --arg cat "$cat" \
            --arg url "$url" \
            --argjson ms "$ms" \
            --argjson won "$won" \
            --arg val "$val" \
            '{
                ts: $ts,
                cat: $cat,
                url: $url,
                ms: $ms,
                won: $won,
                val: $val
            }' \
            >>"$stats_file" 2>/dev/null || true
    }

    # Race multiple URLs in parallel; first to complete with data wins
    # Usage: fetch_race category output_file global_opts url1 url2 ...
    # global_opts: curl options like "-4" or "-6" applied to all URLs
    # Per-URL headers defined in url_headers associative array
    fetch_race() {
        local category=$1 out=$2 global_opts=$3
        shift 3
        local urls=("$@")
        local tmps=() pids=()
        local start_ms
        start_ms=$(($(date +%s%N 2>/dev/null || echo 0) / 1000000))

        for url in "${urls[@]}"; do
            local t
            t=$(mktemp)
            tmps+=("$t")
            
            # Build curl command
            local curl_cmd="curl"
            [[ -n "$global_opts" ]] && curl_cmd+=" $global_opts"
            curl_cmd+=" -sL --connect-timeout 3 --max-time 5"
            [[ -n "${url_headers[$url]:-}" ]] && \
                curl_cmd+=" -H '${url_headers[$url]}'"
            curl_cmd+=" '$url'"
            
            eval "$curl_cmd" >"$t" 2>/dev/null &
            pids+=($!)
        done

        # Poll for first completed result
        while true; do
            local running=0
            for i in "${!pids[@]}"; do
                [[ -z "${pids[$i]}" ]] && continue

                if kill -0 "${pids[$i]}" 2>/dev/null; then
                    running=$((running + 1))
                else
                    # Process finished - log stats
                    local end_ms elapsed_ms val
                    end_ms=$(($(date +%s%N 2>/dev/null || echo 0) / 1000000))
                    elapsed_ms=$((end_ms - start_ms))
                    val=$(tr -d '\r\n' <"${tmps[$i]}" 2>/dev/null || true)

                    if [[ -s "${tmps[$i]}" ]]; then
                        # Save winner URL for extraction later
                        echo "${urls[$i]}" >"${out}.url"
                        # Apply extractor if defined, otherwise use raw data
                        extract_data "${urls[$i]}" "$category" "${tmps[$i]}" >"$out"
                        log_stat "$category" "${urls[$i]}" "$elapsed_ms" true "$val"
                        rm -f "${tmps[$i]}" 2>/dev/null
                        pids[$i]=""

                        # Background logger for remaining processes
                        (
                            for j in "${!pids[@]}"; do
                                [[ -z "${pids[$j]}" ]] && continue
                                wait "${pids[$j]}" 2>/dev/null || true
                                local e_ms v
                                e_ms=$(( $(date +%s%N 2>/dev/null || echo 0) / 1000000 - start_ms ))
                                v=$(tr -d '\r\n' <"${tmps[$j]}" 2>/dev/null || true)
                                log_stat "$category" "${urls[$j]}" "$e_ms" false "$v"
                                rm -f "${tmps[$j]}" 2>/dev/null
                            done
                        ) &
                        return 0
                    fi

                    log_stat "$category" "${urls[$i]}" "$elapsed_ms" false "$val"
                    pids[$i]=""
                fi
            done

            [[ $running -eq 0 ]] && break
            sleep 0.005
        done

        rm -f "${tmps[@]}" 2>/dev/null
    }

    tmp_file=$(mktemp "${cache_dir}/internet.env.XXXXXX")

    # Run all three categories in parallel, each racing multiple services
    fetch_race "pub4" "${tmp_file}.pub4" "-4" \
        "https://speed.cloudflare.com/meta" \
        "https://ifconfig.co" \
        "https://icanhazip.com" \
        "https://api.ipify.org" \
        "https://ifconfig.me" \
        "https://ip.me" &

    fetch_race "pub6" "${tmp_file}.pub6" "-6" \
        "https://speed.cloudflare.com/meta" \
        "https://ifconfig.co" \
        "https://icanhazip.com" \
        "https://api6.ipify.org" \
        "https://ifconfig.me" \
        "https://ip.me" &

    fetch_race "isp" "${tmp_file}.isp" "" \
        "https://speed.cloudflare.com/meta" \
        "https://ifconfig.co/asn-org" \
        "https://ipinfo.io/org" \
        "http://ip-api.com/line/?fields=org" \
        "https://ipapi.co/org/" &

    wait

    # Results are already extracted by fetch_race
    pub4=$(tr -d '\r\n' <"${tmp_file}.pub4" 2>/dev/null || true)
    pub6=$(tr -d '\r\n' <"${tmp_file}.pub6" 2>/dev/null || true)
    isp=$(tr -d '\r\n' <"${tmp_file}.isp" 2>/dev/null || true)

    {
        [[ -n "$pub4" ]] && printf '%s\n' "pub4=${pub4}"
        [[ -n "$pub6" ]] && printf '%s\n' "pub6=${pub6}"
        [[ -n "$isp" ]] && printf '%s\n' "isp=${isp}"
        printf '%s\n' "epoch=$(date +%s)"
    } >"$tmp_file"

    mv -f "$tmp_file" "$cache_file"

    rm -f "${tmp_file}".{pub4,pub6,isp}{,.url} 2>/dev/null || true
    rmdir "$lock_dir" 2>/dev/null || true
) </dev/null >/dev/null 2>&1 &

exit 0

